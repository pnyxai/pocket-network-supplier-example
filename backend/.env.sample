# Your HuggingFace token, vLLM will use this to download the model if needed
HF_TOKEN=hf_zzzzzzz
# Where you want to keep the models in the host machine
MODELS_PATH=/your/host/huggingface_hub/
# The model to deploy (this is just a tutorial, aim for 30B and up for this service)
MODEL_NAME=meta-llama/Llama-3.2-1B-Instruct
# Some configs, feel free to change
DTYPE=auto
GPU_MEMORY_UTILIZATION=0.90
MAX_MODEL_LEN=4096
# This will be mounted later in the docker compose
PONCHO_CONFIG_FILE=./sidecar.yaml